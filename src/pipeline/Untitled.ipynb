{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a673a8c4",
   "metadata": {},
   "source": [
    "## Testing the pipeline as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ea1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295c698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting static data....\n",
      "Ingesting area data....\n",
      "Ingesting cases data....\n",
      "Ingesting mobility data....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3398f1f2ec014747be0706e7f29983ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import from local data files\n",
    "current_path = os.path.abspath('.')\n",
    "sys.path.append(os.path.dirname(current_path))\n",
    "\n",
    "from data_access.data_factory import DataFactory as factory\n",
    "from utils import config as cf\n",
    "from utils import data as data\n",
    "from utils import model as md\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "#####################################\n",
    "#### PARAMETERS FROM CONFIG FILE ####\n",
    "#####################################\n",
    "\n",
    "\n",
    "''' Number of different combinations of grid search hyperparameters\n",
    "Default is 500, use a lower value, >=1 to speed-up the evaluations\n",
    "Will only be used when regression with regularisation model is used\n",
    "for making predictions'''\n",
    "    \n",
    "# Only required if model uses regularisation\n",
    "parm_spce_grid_srch=cf.parm_spce_grid_srch\n",
    "\n",
    "# Create a list of alphas to cross-validate against\n",
    "alphas_val = cf.alphas_val\n",
    "\n",
    "\n",
    "#When should the weekly training start from\n",
    "strt_training_period=cf.chsen_datum\n",
    "\n",
    "#Flag to set if one chooses Zero inflated regression model\n",
    "\n",
    "'''By default, the flag is reset as the architecture of\n",
    "the model is more suited for tranches of low prevalence\n",
    "when there are excessively large number of LSOAs with zero\n",
    "reported cases. This architecture is not suitable for longer\n",
    "time periods where the dataset is less likely to have excessive\n",
    "number of zero reported cases\n",
    "'''\n",
    "\n",
    "zero_inf_flg_st=cf.zero_infltd_modl\n",
    "\n",
    "'''By default, the flag is reset which means\n",
    "regression with regularisation model will be\n",
    "used for making predictions (sklearn package). Do note: irrespective \n",
    "of the status of the flag, the significant coefficients \n",
    "will be through linear regression model (statsmodel package)\n",
    "'''\n",
    "lin_regr_or_regrlsn=cf.linear_rgr_flg\n",
    "\n",
    "################\n",
    "### INGESTS ####\n",
    "################\n",
    "\n",
    "# static variables\n",
    "print(\"Ingesting static data....\")\n",
    "static_df = factory.get('static_vars_for_modelling').create_dataframe()\n",
    "\n",
    "# LSOA Area information to normalise footfall\n",
    "print(\"Ingesting area data....\")\n",
    "area_df = factory.get('static_subset_for_norm').create_dataframe()\n",
    "\n",
    "# Cases data\n",
    "print(\"Ingesting cases data....\")\n",
    "cases_df = factory.get('aggregated_tests_lsoa').create_dataframe()\n",
    "\n",
    "# Mobility data\n",
    "print(\"Ingesting mobility data....\")\n",
    "deimos_footfall_df = factory.get('lsoa_daily_footfall').create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c15142e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique LSOAs in various training weeks [32844]\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:264: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:244: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:284: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##############################\n",
    "#### PROCESS STATIC DATA #####\n",
    "##############################\n",
    "\n",
    "### DROP THE FEATURES THAT THE CORRELATION ANALYSIS SHOWED THAT WE DON'T NEED\n",
    "### FILTER FOR NUMERIC COLUMNS ONLY\n",
    "### AGE COLUMNS ARE DROPPED\n",
    "\n",
    "static_df_num=static_df.set_index(['LSOA11CD','travel_cluster'])\n",
    "static_df_num=static_df_num.select_dtypes(include=np.number)\n",
    "static_df_num=static_df_num[[x for x in static_df_num.columns if x!='Area']]\n",
    "\n",
    "\n",
    "### SELECT THE REQUIRED COLUMNS ###\n",
    "\n",
    "sel_colmns_set = ['CENSUS_2011_ASIAN_ASIAN_BRITISH','METHOD_OF_TRAVEL_TO_WORK_NON_MOTORISED','METHOD_OF_TRAVEL_TO_WORK_Public_TRANSPORT',\\\n",
    "                'METHOD_OF_TRAVEL_TO_WORK_WORK_MAINLY_FROM_HOME','FAMILIES_WITH_DEPENDENT_CHILDREN_NO_DEPENDENT_CHILDREN',\\\n",
    "                'care', 'meat_and_fish_processing','ready_meals', 'textiles', 'warehousing']\n",
    "\n",
    "\n",
    "static_df_num=static_df_num[sel_colmns_set]\n",
    "\n",
    "assert static_df_num.shape == (cf.n_lsoa, 10)\n",
    "\n",
    "\n",
    "##############################\n",
    "#### PROCESS AREA DATA #####\n",
    "##############################\n",
    "\n",
    "norm_area_df=area_df[['LSOA11CD','ALL_PEOPLE','Area']]\n",
    "\n",
    "#### JOIN TO STATIC DATA ####\n",
    "static_df_num=static_df_num.reset_index()\n",
    "static_df_num=static_df_num.merge(norm_area_df,on=['LSOA11CD'],how='inner')\n",
    "\n",
    "### NORMALISE ###\n",
    "\n",
    "# Normalise the IDBR variables: \n",
    "'''expressed as number of residents per unit area\n",
    "who are working in high risk industries'''\n",
    "fctr=1\n",
    "idbr_norm='Area'\n",
    "static_df_num['care']=(static_df_num['care'].div(static_df_num[idbr_norm]))*(fctr)\n",
    "static_df_num['meat_and_fish_processing']=(static_df_num['meat_and_fish_processing'].div(static_df_num[idbr_norm]))*(fctr)\n",
    "static_df_num['textiles']=(static_df_num['textiles'].div(static_df_num[idbr_norm]))*(fctr)\n",
    "static_df_num['ready_meals']=(static_df_num['ready_meals'].div(static_df_num[idbr_norm]))*(fctr)\n",
    "static_df_num['warehousing']=(static_df_num['warehousing'].div(static_df_num[idbr_norm]))*(fctr)\n",
    "\n",
    "static_df_num.drop(columns=['ALL_PEOPLE','Area'],inplace=True)\n",
    "\n",
    "static_df_num.set_index(['LSOA11CD','travel_cluster'],inplace=True)\n",
    "\n",
    "assert static_df_num.shape == (cf.n_lsoa, 10)\n",
    "\n",
    "######### IMPLEMENT THE RESULTS OF THE EXPLORATORY FACTOR ANALYSIS ############\n",
    "\n",
    "\n",
    "'''Based on Factors obtained earlier \n",
    "we combine features as follows'''\n",
    "\n",
    "risk_ftrs=['meat_and_fish_processing', 'textiles', 'ready_meals','warehousing','care']\n",
    "\n",
    "# features except high risk industry features\n",
    "sep_ftrs=[x for x in static_df_num.columns if x not in risk_ftrs]\n",
    " \n",
    "\n",
    "df_sep=static_df_num[sep_ftrs].reset_index()\n",
    "\n",
    "df_risk=static_df_num[risk_ftrs].reset_index()\n",
    "\n",
    "\n",
    "'''df_risk_sum captures 'interactions'\n",
    "between IDBR features\n",
    "'''\n",
    "\n",
    "df_risk_sum=df_risk.select_dtypes(include=object)\n",
    "\n",
    "### CREATE THE NEW COLUMNS\n",
    "\n",
    "df_risk_sum.loc[:,'care_homes_warehousing_textiles']=df_risk[['LSOA11CD', 'travel_cluster','textiles', 'warehousing', 'care']].sum(axis=1)\n",
    "\n",
    "df_risk_sum.loc[:,'meat_and_fish_processing']=df_risk[['LSOA11CD', 'travel_cluster','meat_and_fish_processing']].sum(axis=1)\n",
    "\n",
    "df_risk_sum.loc[:,'ready_meals']=df_risk[['LSOA11CD', 'travel_cluster','ready_meals']].sum(axis=1)\n",
    "\n",
    "### MERGE THE SEPARATE FEATURES WITH THE NEW RISK FEATURES\n",
    "\n",
    "list_dfs=[df_sep,df_risk_sum]\n",
    "\n",
    "static_df_new_variables = reduce(lambda left,right: pd.merge(left,right,on=['LSOA11CD','travel_cluster']), list_dfs)\n",
    "\n",
    "# This is the final static dataframe - still neeed to add cases and dynamic data\n",
    "static_df_new_variables=static_df_new_variables.merge(norm_area_df,how='inner',on=['LSOA11CD']).reset_index(drop=True)\n",
    "\n",
    "assert static_df_new_variables.shape == (cf.n_lsoa, 12)\n",
    "\n",
    "\n",
    "#################################\n",
    "#### PROCESS THE CASES DATA #####\n",
    "#################################\n",
    "\n",
    "# sort values by date\n",
    "cases_df_datum=cases_df[['Date','LSOA11CD','COVID_Cases']].sort_values(by='Date').reset_index(drop=True)\n",
    "    \n",
    "# create a list of dataframes of cases, one df for each week \n",
    "cases_df_datum=[pd.DataFrame(y) for x, y in cases_df_datum.groupby('Date', as_index=False)]\n",
    "    \n",
    "cases_df_datum_mrgd=[]\n",
    "\n",
    "# The cases dataframe is split into different dates.\n",
    "# This splitting allows for each dataframe to be left joined to the static data\n",
    "# Therefore there will be a cases value for every LSOA for every week\n",
    "# If no cases data is present for a given week in a given LSOA, the 'Date'\n",
    "# field is filled with the 'Date' value from that DataFrame\n",
    "\n",
    "\n",
    "# for each df\n",
    "for splt_df in cases_df_datum:\n",
    "    \n",
    "    # store the date for the given DataFrame\n",
    "    datm=splt_df['Date'].unique()[0]\n",
    "    \n",
    "    # left-join cases onto the static data\n",
    "    df=static_df_new_variables.merge(splt_df,how='left',on=['LSOA11CD'])\n",
    "    \n",
    "    # fill any gaps in the cases data with the correct date\n",
    "    df['Date']=df['Date'].fillna(datm)\n",
    "    \n",
    "    # any dates that needed to be filled had zero cases for that week\n",
    "    df['COVID_Cases']=df['COVID_Cases'].fillna(0)\n",
    "    \n",
    "    # apply normalisation\n",
    "    df['COVID_Cases']=df['COVID_Cases'].div(df['Area'])\n",
    "    \n",
    "    cases_df_datum_mrgd.append(df)\n",
    "        \n",
    "# stack the dataframes        \n",
    "df_all_tranches_sbset=pd.concat(cases_df_datum_mrgd).reset_index(drop=True)\n",
    "\n",
    "# drop the area column\n",
    "df_all_tranches_sbset.drop('Area', axis=1, inplace=True)\n",
    "\n",
    "# rename to reflect normalisation\n",
    "df_all_tranches_sbset.rename(columns={'COVID_Cases':'COVID_Cases_per_unit_area'},inplace=True)\n",
    "\n",
    "assert df_all_tranches_sbset.shape == (2627520, 13)\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "#### CHOOSE THE MODEL TRAINING PERIOD ####\n",
    "##########################################\n",
    "\n",
    "\n",
    "# Change this filter if different time period \n",
    "# is required to infer respective risk predictors\n",
    "\n",
    "df_all_tranches_sbset=df_all_tranches_sbset[df_all_tranches_sbset['Date']>=strt_training_period]\n",
    "\n",
    "#### GENERATE A 'WEEK' COLUMN ###\n",
    "\n",
    "date_list=sorted(df_all_tranches_sbset['Date'].dt.date.unique())\n",
    "    \n",
    "week_list=[\"week_\"+str(x+1) for x in range(len(date_list))]\n",
    "    \n",
    "date_dict=dict(zip(date_list,week_list))\n",
    "    \n",
    "df_all_tranches_sbset['week']=df_all_tranches_sbset['Date'].map(date_dict)\n",
    "\n",
    "df_all_tranches_sbset['Date']=df_all_tranches_sbset['Date'].astype(str)\n",
    "\n",
    "df_all_tranches_sbset=df_all_tranches_sbset.reset_index(drop=True)\n",
    "\n",
    "# This is to visually check we have same number of LSOAs in consecutive weeks of training data\n",
    "print('Unique LSOAs in various training weeks {}'.format(df_all_tranches_sbset.groupby('Date')['LSOA11CD'].count().unique()))\n",
    "\n",
    "assert df_all_tranches_sbset.groupby('Date')['LSOA11CD'].count().unique() == cf.n_lsoa\n",
    "\n",
    "\n",
    "##############################\n",
    "### PROCESS MOBILITY DATA ####\n",
    "##############################\n",
    "\n",
    "deimos_footfall_df['Date']=deimos_footfall_df['Date'].astype(str)\n",
    "\n",
    "# Issue: December-2021: CJ\n",
    "# Since the delay in cases data available to us is longer than the \n",
    "# delay in regular ingestion of mobility data- we use the 'excess'\n",
    "# mobility data alongside the static predictors to predict the number\n",
    "# of cases--This is one way to validate the results\n",
    "\n",
    "# Dataset containing both the static and dynamic predictors alongside the target variable\n",
    "df_all_tranches_sbset=df_all_tranches_sbset.merge(deimos_footfall_df,how='inner',on=['LSOA11CD','Date'])\n",
    "\n",
    "# This is to visually check we have same number of LSOAs in consecutive weeks of training data (including both static and dynamic predictors)\n",
    "# We should not lose any LSOAS: this value should be 32844\n",
    "assert df_all_tranches_sbset.groupby('Date')['LSOA11CD'].count().unique() == cf.n_lsoa\n",
    "\n",
    "\n",
    "#### CREATE TEST SET #####\n",
    "\n",
    "# These are the dates for which we have mobility data but we don't have cases data\n",
    "\n",
    "# Test (unseen data) for predicting future cases: we only capture the predictors in this dataset\n",
    "df_all_tranches_sbset_tst_data=deimos_footfall_df.merge(static_df_new_variables,how='inner',on=['LSOA11CD'])\n",
    "\n",
    "# Test dataset should contain timestamps not present in the training data\n",
    "df_all_tranches_sbset_tst_data=df_all_tranches_sbset_tst_data[df_all_tranches_sbset_tst_data['Date']>df_all_tranches_sbset['Date'].max()].reset_index(drop=True)\n",
    "\n",
    "assert df_all_tranches_sbset_tst_data.groupby('Date')['LSOA11CD'].count().unique() == cf.n_lsoa\n",
    "\n",
    "#Test data date range\n",
    "tst_dat_rng=df_all_tranches_sbset_tst_data['Date'].min()+'-'+df_all_tranches_sbset_tst_data['Date'].max()\n",
    "\n",
    "# Drop columns we don't need\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#### PROCESS THE DATA INTO TRANCHES ####\n",
    "########################################\n",
    "\n",
    "tranches_uk = cf.tranches_uk\n",
    "events = cf.events\n",
    "\n",
    "splt_df_tranches=[]\n",
    "\n",
    "for tim_slice in range(len(tranches_uk)):\n",
    "    print(tim_slice)\n",
    "    \n",
    "    # if tim_slice is the final element of the list\n",
    "    if tim_slice == len(tranches_uk)-1:\n",
    "        \n",
    "        # t1 is the selected date\n",
    "        t1 = tranches_uk[tim_slice]\n",
    "        \n",
    "        # subset for all dates after t1\n",
    "        df_tim = df_all_tranches_sbset[df_all_tranches_sbset['Date']>t1]\n",
    "        \n",
    "        # if dataframe is not empty\n",
    "        if df_tim.shape[0] != 0:\n",
    "            \n",
    "            # add a column for the event description\n",
    "            df_tim['tranche_desc']=events[tim_slice]\n",
    "        \n",
    "        splt_df_tranches.append(df_tim)\n",
    "    \n",
    "    # if tim_slice is not the final element of the list\n",
    "    else:\n",
    "        \n",
    "        # t1 is the selected date\n",
    "        t1 = tranches_uk[tim_slice]\n",
    "        \n",
    "        # t2 is the next date in the list\n",
    "        t2 = tranches_uk[tim_slice+1]\n",
    "        \n",
    "        # return the data between t1 and t2\n",
    "        df_tim = data.split_time_slice(df_all_tranches_sbset,t1,t2)\n",
    "        \n",
    "        # if dataframe is not empty\n",
    "        if df_tim.shape[0] != 0:\n",
    "            \n",
    "            # add a column for the event description\n",
    "            df_tim['tranche_desc'] = events[tim_slice]\n",
    "        \n",
    "        splt_df_tranches.append(df_tim)\n",
    "        \n",
    "        \n",
    "\n",
    "# remove sliced df for which there is no data available \n",
    "# (mobilty data is available from tranche 2 onwards, so the first sliced df will be empty)\n",
    "splt_df_tranches=[x for x in splt_df_tranches if len(x)!=0]\n",
    "\n",
    "\n",
    "# Perform aggregation of predictors and target variable for each tranche\n",
    "# Each tranche contains multiple weeks, aggregation results in mean of each of the numerical features\n",
    "# In practice, the static features are the same for each week, so we are averaging footfall over the tranche\n",
    "# Each sliced df will have one unique record for each LSOA (because of averaging)\n",
    "splt_df_tranches_agg=[]\n",
    "\n",
    "for df_x in splt_df_tranches:\n",
    "    \n",
    "    # convert date column to string showing date range of the tranche\n",
    "    df_x['Date']=str(df_x['Date'].min())+'-'+str(df_x['Date'].max())\n",
    "    \n",
    "    # define columns to group by\n",
    "    grpp_colms=['Date','LSOA11CD','tranche_desc','travel_cluster']\n",
    "    \n",
    "    # compute the mean over each week in the tranche\n",
    "    df_x=df_x.groupby(grpp_colms)[[x for x in df_x.columns if x not in grpp_colms]].mean().reset_index()\n",
    "    \n",
    "    # sort by LSOA code\n",
    "    df_x=df_x.sort_values(by='LSOA11CD').reset_index(drop=True)\n",
    "    \n",
    "    splt_df_tranches_agg.append(df_x)\n",
    "    \n",
    "\n",
    "# stack each tranche into one dataframe\n",
    "splt_df_tranches_conct=pd.concat(splt_df_tranches_agg).reset_index(drop=True)\n",
    "\n",
    "# convert date to string\n",
    "splt_df_tranches_conct['Date']=splt_df_tranches_conct['Date'].astype(str)\n",
    "\n",
    "# find unique date range and tranche description combinations\n",
    "df_key=splt_df_tranches_conct[['Date','tranche_desc']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# put them into a dictionary\n",
    "event_dict=dict(zip(df_key['Date'].values,df_key['tranche_desc'].values))\n",
    "\n",
    "# list of integers from 1 to n_tranches inclusive \n",
    "tranche_order=list(range(1, cf.n_tranches + 1))\n",
    "\n",
    "# zip the tranche descriptions and tranche numbers\n",
    "event_order_dict=dict(zip(events,tranche_order))\n",
    "\n",
    "# dict of tranche number: tranche description\n",
    "rvse_event_dict={v: k for k, v in event_order_dict.items()}\n",
    "\n",
    "# dict of tranche description: tranche date range\n",
    "rvse_date_dict={v: k for k, v in event_dict.items()}\n",
    "\n",
    "# the dataframe with all tranches\n",
    "df_all_tranches_sbset=splt_df_tranches_conct\n",
    "\n",
    "# create new column for tranche number\n",
    "df_all_tranches_sbset['tranche_order']=df_all_tranches_sbset['tranche_desc'].map(event_order_dict)\n",
    "\n",
    "# test that each LSOA appears one for each tranche\n",
    "# there is no mobility data for the first tranch, therefore the shape should be (n_tranches - 1) * n_lsoa\n",
    "assert df_all_tranches_sbset.shape == (((cf.n_tranches - 1) * cf.n_lsoa), 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a3e3266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#### MORE MOBILITY PROCESSING ####\n",
    "##################################\n",
    "\n",
    "# COLUMNS TO DROP BEFORE MODELLING\n",
    "cols_to_drop=['ALL_PEOPLE','msoa_people', 'resident_footfall_sqkm','total_footfall_sqkm','visitor_footfall_sqkm','worker_footfall_sqkm']\n",
    "\n",
    "#Remove un-necessary columns from training and test data\n",
    "df_all_tranches_sbset.drop(columns = cols_to_drop, inplace=True)\n",
    "df_all_tranches_sbset_tst_data.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#convert mobility features into per sq metres and rename columns\n",
    "fctr_mob = 0.000001\n",
    "\n",
    "df_all_tranches_sbset[['worker_visitor_footfall_sqkm']] = df_all_tranches_sbset[['worker_visitor_footfall_sqkm']] * (fctr_mob)\n",
    "\n",
    "df_all_tranches_sbset.rename(columns={'worker_visitor_footfall_sqkm':'worker_visitor_footfall_sqm'}\n",
    "                                     , inplace=True)\n",
    "\n",
    "\n",
    "df_all_tranches_sbset_tst_data[['worker_visitor_footfall_sqkm']] = df_all_tranches_sbset_tst_data[['worker_visitor_footfall_sqkm']] * (fctr_mob)\n",
    "\n",
    "df_all_tranches_sbset_tst_data.rename(columns={'worker_visitor_footfall_sqkm':'worker_visitor_footfall_sqm'}\n",
    "                                      ,inplace=True)\n",
    "\n",
    "\n",
    "###############################\n",
    "#### PROCESS IDBR FEATURES ####\n",
    "###############################\n",
    "\n",
    "#convert idbr features into per hectare (they were earlier expressed as per unit area (sq km): 1 sq km= 100 hectare)\n",
    "\n",
    "fctr_risk = 0.01\n",
    "\n",
    "df_all_tranches_sbset['care_homes_warehousing_textiles']=(df_all_tranches_sbset['care_homes_warehousing_textiles'])*(fctr_risk)\n",
    "df_all_tranches_sbset['meat_and_fish_processing']=(df_all_tranches_sbset['meat_and_fish_processing'])*(fctr_risk)\n",
    "df_all_tranches_sbset['ready_meals']=(df_all_tranches_sbset['ready_meals'])*(fctr_risk)\n",
    "\n",
    "\n",
    "df_all_tranches_sbset_tst_data['care_homes_warehousing_textiles']=(df_all_tranches_sbset_tst_data['care_homes_warehousing_textiles'])*(fctr_risk)\n",
    "df_all_tranches_sbset_tst_data['meat_and_fish_processing']=(df_all_tranches_sbset_tst_data['meat_and_fish_processing'])*(fctr_risk)\n",
    "df_all_tranches_sbset_tst_data['ready_meals']=(df_all_tranches_sbset_tst_data['ready_meals'])*(fctr_risk)\n",
    "\n",
    "\n",
    "# Aggregate test data-- so unique value for each LSOA\n",
    "df_all_tranches_sbset_tst_data=df_all_tranches_sbset_tst_data.groupby(['LSOA11CD', 'travel_cluster'])[list(df_all_tranches_sbset_tst_data.\\\n",
    "                                                                                                           select_dtypes(include=np.number).columns)].mean().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0c0ddda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1. >70% metropolitan core dwellers\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.2748370896838259\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.2769252389958712\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.6877094909243264\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.09265161878476358\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.22214055908578623\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5258887366044042\n",
      "L2. >70% outer metropolitan dwellers\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.14275426931438873\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5155455130792441\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.6459720647149122\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.3934370673242311\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.22150276976472194\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.22347772606685312\n",
      "L3. >70% suburban dwellers\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.32604130790715735\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.46013409604160116\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.44689195445204366\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5101548952213855\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.4259559636511572\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5072664234030324\n",
      "L4. >70% exurban dwellers\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.31649730007592947\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.4310010480347676\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5897435327985567\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.40762853915670805\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.4461829154883209\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.7150684698525749\n",
      "L5. >70% rural dwellers\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.312401453961473\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5535312774493644\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.7094494607688329\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.2369730120539436\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.5740426946259296\n",
      "CV starts without zero-inflated model and with regularisation\n",
      "CV finishes without zero-inflated model and with regularisation\n",
      "r2_score: train score=0.7466472533965101\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#### MODELLING BEGINS ####\n",
    "##########################\n",
    "\n",
    "\n",
    "# Create list of travel clusters\n",
    "list_of_tc=sorted(df_all_tranches_sbset['travel_cluster'].unique())\n",
    "'''Separate regression model is fit for each travel cluster- multi-group analysis\n",
    "This is done to further account for any spatial correlation amongst the features'''\n",
    "\n",
    "# Lists to store the outputs\n",
    "str_pred_tc_static=[]\n",
    "str_coef_tc_static=[]\n",
    "str_se_coef_tc_static=[]\n",
    "str_non_se_coef_tc_static=[]\n",
    "str_pred_tc_recnt=[]\n",
    "\n",
    "# for each travel cluster\n",
    "for sbset_tc in list_of_tc:\n",
    "    print(sbset_tc)\n",
    "    \n",
    "    # subset the training data for the travel cluster\n",
    "    df_chsen=df_all_tranches_sbset[df_all_tranches_sbset['travel_cluster']==sbset_tc].reset_index(drop=True)\n",
    "    \n",
    "    # sort by tranche\n",
    "    df_chsen=df_chsen.sort_values(by=['tranche_order','LSOA11CD']).reset_index(drop=True)\n",
    "    \n",
    "    # drop the columns of strings\n",
    "    df_chsen=df_chsen[[x for x in df_chsen.columns if x not in ['tranche_desc','Date']]]\n",
    "    \n",
    "    # pass the training data, test data and parameters from the config file to the modelling function\n",
    "    pred_tc,coef_tc,se_tc,non_se_tc,pred_tst_tc=md.fit_model_tranche_static_dynamic(lin_regr_or_regrlsn,df_chsen,zero_inf_flg_st,alphas_val,parm_spce_grid_srch,\\\n",
    "                                                                                    df_all_tranches_sbset_tst_data)\n",
    "    \n",
    "    # store the results\n",
    "    str_pred_tc_static.append(pred_tc)\n",
    "    str_coef_tc_static.append(coef_tc) \n",
    "    str_se_coef_tc_static.append(se_tc)\n",
    "    str_non_se_coef_tc_static.append(non_se_tc)\n",
    "    str_pred_tc_recnt.append(pred_tst_tc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "535a2a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSOA with negative predictions on train data...\n",
      "count    3440.000000\n",
      "mean       -0.541308\n",
      "std         1.294177\n",
      "min       -32.786048\n",
      "25%        -0.504810\n",
      "50%        -0.200492\n",
      "75%        -0.079751\n",
      "max        -0.000040\n",
      "Name: Predicted_cases_train, dtype: float64\n",
      "LSOA with negative predictions on unseen test data...\n",
      "count    577.000000\n",
      "mean      -0.574884\n",
      "std        0.801976\n",
      "min       -8.755973\n",
      "25%       -0.655099\n",
      "50%       -0.316137\n",
      "75%       -0.140799\n",
      "max       -0.000633\n",
      "Name: Predicted_cases_test, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Store most important features used for making predictions \n",
    "# for each travel cluster and for each tranche\n",
    "# This includes both significant and non-significant features\n",
    "\n",
    "# Model coefficients (with regularisation)\n",
    "str_coef_tc_static=pd.concat(str_coef_tc_static).reset_index()\n",
    "\n",
    "# Model predictions (with regularisation)\n",
    "str_pred_tc_static=pd.concat(str_pred_tc_static).reset_index(drop=True)\n",
    "\n",
    "# Store the standard error/p-value of the coefficients of trained model\n",
    "# (No regularisation, standardised coefs)\n",
    "str_se_coef_tc_static=pd.concat(str_se_coef_tc_static).reset_index()\n",
    "\n",
    "# Store the standard error/p-value of the coefficients of trained model\n",
    "# (No regularisation, non-standardised coefs)\n",
    "str_non_se_coef_tc_static=pd.concat(str_non_se_coef_tc_static).reset_index()\n",
    "\n",
    "# Store the predictions of trained model (on unseen test data)\n",
    "pred_latest=pd.concat(str_pred_tc_recnt).sort_values(by='Predicted_cases_test',ascending=False).reset_index(drop=True)\n",
    "\n",
    "pred_latest['Date']=tst_dat_rng\n",
    "\n",
    "# Add  \n",
    "pred_latest=pred_latest.merge(area_df,on=['LSOA11CD','travel_cluster','ALL_PEOPLE','Area'],how='inner').sort_values(by='Predicted_cases_test',ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Round negative predictions to zero\n",
    "pred_latest['Predicted_cases_test']=pred_latest['Predicted_cases_test'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a285c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dfs = [str_coef_tc_static, str_se_coef_tc_static, str_non_se_coef_tc_static]\n",
    "\n",
    "for df in output_dfs:\n",
    "    \n",
    "    df['Date'] = df['tranche'].map(rvse_event_dict).map(rvse_date_dict)\n",
    "    df['tranche_desc'] = df['tranche'].map(rvse_date_dict)\n",
    "    df['Features'] = df['Features'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ef27b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual_cases</th>\n",
       "      <th>Predicted_cases_train</th>\n",
       "      <th>tranche_train</th>\n",
       "      <th>travel_cluster</th>\n",
       "      <th>LSOA11CD</th>\n",
       "      <th>Best_cv_score_train</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>Probability_of_COVID_Case_train</th>\n",
       "      <th>Predicted_Class_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.281131</td>\n",
       "      <td>47.433315</td>\n",
       "      <td>2</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>E01000001</td>\n",
       "      <td>0.303386</td>\n",
       "      <td>30.261590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.064929</td>\n",
       "      <td>43.225596</td>\n",
       "      <td>2</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>E01000002</td>\n",
       "      <td>0.303386</td>\n",
       "      <td>30.261590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.371136</td>\n",
       "      <td>72.428699</td>\n",
       "      <td>2</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>E01000003</td>\n",
       "      <td>0.303386</td>\n",
       "      <td>30.261590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.350923</td>\n",
       "      <td>54.024138</td>\n",
       "      <td>2</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>E01000005</td>\n",
       "      <td>0.303386</td>\n",
       "      <td>30.261590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.914926</td>\n",
       "      <td>16.572683</td>\n",
       "      <td>2</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>E01000842</td>\n",
       "      <td>0.303386</td>\n",
       "      <td>30.261590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197059</th>\n",
       "      <td>0.090226</td>\n",
       "      <td>-0.306116</td>\n",
       "      <td>7</td>\n",
       "      <td>L5. &gt;70% rural dwellers</td>\n",
       "      <td>E01033529</td>\n",
       "      <td>0.701830</td>\n",
       "      <td>1.780165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197060</th>\n",
       "      <td>0.043312</td>\n",
       "      <td>0.138412</td>\n",
       "      <td>7</td>\n",
       "      <td>L5. &gt;70% rural dwellers</td>\n",
       "      <td>E01033530</td>\n",
       "      <td>0.701830</td>\n",
       "      <td>1.780165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197061</th>\n",
       "      <td>5.520434</td>\n",
       "      <td>3.525779</td>\n",
       "      <td>7</td>\n",
       "      <td>L5. &gt;70% rural dwellers</td>\n",
       "      <td>E01033538</td>\n",
       "      <td>0.701830</td>\n",
       "      <td>1.780165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197062</th>\n",
       "      <td>6.643441</td>\n",
       "      <td>4.450748</td>\n",
       "      <td>7</td>\n",
       "      <td>L5. &gt;70% rural dwellers</td>\n",
       "      <td>E01033539</td>\n",
       "      <td>0.701830</td>\n",
       "      <td>1.780165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197063</th>\n",
       "      <td>0.636459</td>\n",
       "      <td>1.781527</td>\n",
       "      <td>7</td>\n",
       "      <td>L5. &gt;70% rural dwellers</td>\n",
       "      <td>E01033609</td>\n",
       "      <td>0.701830</td>\n",
       "      <td>1.780165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197064 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual_cases  Predicted_cases_train  tranche_train  \\\n",
       "0           5.281131              47.433315              2   \n",
       "1           6.064929              43.225596              2   \n",
       "2          47.371136              72.428699              2   \n",
       "3           2.350923              54.024138              2   \n",
       "4          13.914926              16.572683              2   \n",
       "...              ...                    ...            ...   \n",
       "197059      0.090226              -0.306116              7   \n",
       "197060      0.043312               0.138412              7   \n",
       "197061      5.520434               3.525779              7   \n",
       "197062      6.643441               4.450748              7   \n",
       "197063      0.636459               1.781527              7   \n",
       "\n",
       "                             travel_cluster   LSOA11CD  Best_cv_score_train  \\\n",
       "0       L1. >70% metropolitan core dwellers  E01000001             0.303386   \n",
       "1       L1. >70% metropolitan core dwellers  E01000002             0.303386   \n",
       "2       L1. >70% metropolitan core dwellers  E01000003             0.303386   \n",
       "3       L1. >70% metropolitan core dwellers  E01000005             0.303386   \n",
       "4       L1. >70% metropolitan core dwellers  E01000842             0.303386   \n",
       "...                                     ...        ...                  ...   \n",
       "197059              L5. >70% rural dwellers  E01033529             0.701830   \n",
       "197060              L5. >70% rural dwellers  E01033530             0.701830   \n",
       "197061              L5. >70% rural dwellers  E01033538             0.701830   \n",
       "197062              L5. >70% rural dwellers  E01033539             0.701830   \n",
       "197063              L5. >70% rural dwellers  E01033609             0.701830   \n",
       "\n",
       "        RMSE_train  Probability_of_COVID_Case_train  Predicted_Class_train  \n",
       "0        30.261590                                0                      0  \n",
       "1        30.261590                                0                      0  \n",
       "2        30.261590                                0                      0  \n",
       "3        30.261590                                0                      0  \n",
       "4        30.261590                                0                      0  \n",
       "...            ...                              ...                    ...  \n",
       "197059    1.780165                                0                      0  \n",
       "197060    1.780165                                0                      0  \n",
       "197061    1.780165                                0                      0  \n",
       "197062    1.780165                                0                      0  \n",
       "197063    1.780165                                0                      0  \n",
       "\n",
       "[197064 rows x 9 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_pred_tc_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting back the Date and other columns in the stored outputs\n",
    "\n",
    "output_dfs = [str_coef_tc_static, str_se_coef_tc_static, str_non_se_coef_tc_static]\n",
    "\n",
    "for df in output_dfs:\n",
    "    \n",
    "    df['Date'] = df['tranche'].map(rvse_event_dict).map(rvse_date_dict)\n",
    "    df['tranche_desc'] = df['tranche'].map(rvse_date_dict)   \n",
    "    df['Features'] = df['Features'].str.lower()\n",
    "\n",
    "\n",
    "# For the prediction dataframe\n",
    "str_pred_tc_static['tranche_desc']=str_pred_tc_static['tranche_train'].map(rvse_event_dict)\n",
    "str_pred_tc_static['Date']=str_pred_tc_static['tranche_desc'].map(rvse_date_dict)\n",
    "\n",
    "\n",
    "# For the prediction dataframes, remove the reference to training and rename coefficients\n",
    "str_pred_tc_static.rename(columns={'tranche_train':'tranche'},inplace=True)\n",
    "\n",
    "# For the non-regularisation model\n",
    "str_se_coef_tc_static.rename(columns={'Coefficients':'coef_sklearn'},inplace=True)\n",
    "str_se_coef_tc_static.rename(columns={'coef':'standardised_coef','P>|t|':'P_value','[0.025':'lower_bound','0.975]':'upper_bound','std err':'std_err'},inplace=True)\n",
    "\n",
    "str_non_se_coef_tc_static.rename(columns={'P>|t|':'P_value','[0.025':'lower_bound','0.975]':'upper_bound','std err':'std_err'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#### WRITE RESULTS ####\n",
    "#######################\n",
    "\n",
    "\n",
    "# SAVE OUTPUTS\n",
    "tranches=True\n",
    "    \n",
    "dataset_suffix = '_zir_static_dynamic'\n",
    "if not zero_inf_flg_st:\n",
    "    dataset_suffix = '_no' + dataset_suffix\n",
    "\n",
    "\n",
    "    \n",
    "if tranches:\n",
    "    dataset_suffix=dataset_suffix+'_tranches'\n",
    "else:\n",
    "    dataset_suffix=dataset_suffix+'_no_tranches'   \n",
    "\n",
    "# Regularisation coefs\n",
    "str_coef_tc_static.to_gbq('review_ons.multi_grp_coef' + dataset_suffix, project_id='ons-hotspot-prod',if_exists='replace')\n",
    "\n",
    "# Non-regularisation - standardised coefs\n",
    "str_se_coef_tc_static.to_gbq('review_ons.multi_grp_se_coef' + dataset_suffix, project_id='ons-hotspot-prod',if_exists='replace')\n",
    "\n",
    "# Non-regularisation - non-standardised coefs\n",
    "str_non_se_coef_tc_static.to_gbq('review_ons.multi_grp_se_coef' + dataset_suffix, project_id='ons-hotspot-prod',if_exists='replace')\n",
    "\n",
    "# Predictions for all tranches\n",
    "str_pred_tc_static.to_gbq('review_ons.multi_grp_pred' + dataset_suffix ,project_id='ons-hotspot-prod',if_exists='replace')\n",
    "\n",
    "# Predictions for the latest tranche only\n",
    "pred_latest.to_gbq('review_ons.multi_grp_pred_test_data' + dataset_suffix,project_id='ons-hotspot-prod',if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad2d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25cb4243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSOA11CD</th>\n",
       "      <th>Predicted_cases_test</th>\n",
       "      <th>travel_cluster</th>\n",
       "      <th>ALL_PEOPLE</th>\n",
       "      <th>Area</th>\n",
       "      <th>Date</th>\n",
       "      <th>RGN19NM</th>\n",
       "      <th>UTLA20NM</th>\n",
       "      <th>MSOA11NM</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E01032775</td>\n",
       "      <td>1409.258162</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>London</td>\n",
       "      <td>Tower Hamlets</td>\n",
       "      <td>Tower Hamlets 031</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E01002842</td>\n",
       "      <td>807.528281</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>1012</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>London</td>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>Kensington and Chelsea 021</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E01008506</td>\n",
       "      <td>712.356296</td>\n",
       "      <td>L3. &gt;70% suburban dwellers</td>\n",
       "      <td>1379</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>North East</td>\n",
       "      <td>North Tyneside</td>\n",
       "      <td>North Tyneside 012</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E01013744</td>\n",
       "      <td>553.442305</td>\n",
       "      <td>L3. &gt;70% suburban dwellers</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.026219</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>East Midlands</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>Leicester 018</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E01033487</td>\n",
       "      <td>547.451733</td>\n",
       "      <td>L1. &gt;70% metropolitan core dwellers</td>\n",
       "      <td>1491</td>\n",
       "      <td>0.007451</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>London</td>\n",
       "      <td>Islington</td>\n",
       "      <td>Islington 011</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32839</th>\n",
       "      <td>E01023044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L4. &gt;70% exurban dwellers</td>\n",
       "      <td>1132</td>\n",
       "      <td>1.702108</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>New Forest 023</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32840</th>\n",
       "      <td>E01031621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L4. &gt;70% exurban dwellers</td>\n",
       "      <td>1683</td>\n",
       "      <td>1.863036</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>South East</td>\n",
       "      <td>West Sussex</td>\n",
       "      <td>Horsham 012</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32841</th>\n",
       "      <td>E01018667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L3. &gt;70% suburban dwellers</td>\n",
       "      <td>1652</td>\n",
       "      <td>13.667057</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>North West</td>\n",
       "      <td>Cheshire East</td>\n",
       "      <td>Cheshire East 011</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32842</th>\n",
       "      <td>E01015404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L3. &gt;70% suburban dwellers</td>\n",
       "      <td>1775</td>\n",
       "      <td>1.631246</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>South West</td>\n",
       "      <td>Bournemouth, Christchurch and Poole</td>\n",
       "      <td>Poole 018</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32843</th>\n",
       "      <td>E01012736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L3. &gt;70% suburban dwellers</td>\n",
       "      <td>1261</td>\n",
       "      <td>0.439331</td>\n",
       "      <td>2021-10-17-2021-12-12</td>\n",
       "      <td>North West</td>\n",
       "      <td>Blackpool</td>\n",
       "      <td>Blackpool 010</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32844 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LSOA11CD  Predicted_cases_test                       travel_cluster  \\\n",
       "0      E01032775           1409.258162  L1. >70% metropolitan core dwellers   \n",
       "1      E01002842            807.528281  L1. >70% metropolitan core dwellers   \n",
       "2      E01008506            712.356296           L3. >70% suburban dwellers   \n",
       "3      E01013744            553.442305           L3. >70% suburban dwellers   \n",
       "4      E01033487            547.451733  L1. >70% metropolitan core dwellers   \n",
       "...          ...                   ...                                  ...   \n",
       "32839  E01023044              0.000000            L4. >70% exurban dwellers   \n",
       "32840  E01031621              0.000000            L4. >70% exurban dwellers   \n",
       "32841  E01018667              0.000000           L3. >70% suburban dwellers   \n",
       "32842  E01015404              0.000000           L3. >70% suburban dwellers   \n",
       "32843  E01012736              0.000000           L3. >70% suburban dwellers   \n",
       "\n",
       "       ALL_PEOPLE       Area                   Date        RGN19NM  \\\n",
       "0            1400   0.002385  2021-10-17-2021-12-12         London   \n",
       "1            1012   0.004391  2021-10-17-2021-12-12         London   \n",
       "2            1379   0.012967  2021-10-17-2021-12-12     North East   \n",
       "3            1738   0.026219  2021-10-17-2021-12-12  East Midlands   \n",
       "4            1491   0.007451  2021-10-17-2021-12-12         London   \n",
       "...           ...        ...                    ...            ...   \n",
       "32839        1132   1.702108  2021-10-17-2021-12-12     South East   \n",
       "32840        1683   1.863036  2021-10-17-2021-12-12     South East   \n",
       "32841        1652  13.667057  2021-10-17-2021-12-12     North West   \n",
       "32842        1775   1.631246  2021-10-17-2021-12-12     South West   \n",
       "32843        1261   0.439331  2021-10-17-2021-12-12     North West   \n",
       "\n",
       "                                  UTLA20NM                    MSOA11NM  \\\n",
       "0                            Tower Hamlets           Tower Hamlets 031   \n",
       "1                   Kensington and Chelsea  Kensington and Chelsea 021   \n",
       "2                           North Tyneside          North Tyneside 012   \n",
       "3                                Leicester               Leicester 018   \n",
       "4                                Islington               Islington 011   \n",
       "...                                    ...                         ...   \n",
       "32839                            Hampshire              New Forest 023   \n",
       "32840                          West Sussex                 Horsham 012   \n",
       "32841                        Cheshire East           Cheshire East 011   \n",
       "32842  Bournemouth, Christchurch and Poole                   Poole 018   \n",
       "32843                            Blackpool               Blackpool 010   \n",
       "\n",
       "       Country  \n",
       "0      England  \n",
       "1      England  \n",
       "2      England  \n",
       "3      England  \n",
       "4      England  \n",
       "...        ...  \n",
       "32839  England  \n",
       "32840  England  \n",
       "32841  England  \n",
       "32842  England  \n",
       "32843  England  \n",
       "\n",
       "[32844 rows x 10 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40614520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec1c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2927ca11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSOA11CD</th>\n",
       "      <th>ALL_PEOPLE</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E01011954</td>\n",
       "      <td>2210</td>\n",
       "      <td>0.390643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E01011969</td>\n",
       "      <td>1293</td>\n",
       "      <td>0.838264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E01033465</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.408588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E01011970</td>\n",
       "      <td>1108</td>\n",
       "      <td>0.367279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E01033467</td>\n",
       "      <td>2339</td>\n",
       "      <td>0.415572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32839</th>\n",
       "      <td>E01004738</td>\n",
       "      <td>1879</td>\n",
       "      <td>0.085649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32840</th>\n",
       "      <td>E01004665</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.013524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32841</th>\n",
       "      <td>E01004669</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.100576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32842</th>\n",
       "      <td>E01004739</td>\n",
       "      <td>1598</td>\n",
       "      <td>0.028525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32843</th>\n",
       "      <td>E01004741</td>\n",
       "      <td>1434</td>\n",
       "      <td>0.051046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32844 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LSOA11CD  ALL_PEOPLE      Area\n",
       "0      E01011954        2210  0.390643\n",
       "1      E01011969        1293  0.838264\n",
       "2      E01033465        1915  0.408588\n",
       "3      E01011970        1108  0.367279\n",
       "4      E01033467        2339  0.415572\n",
       "...          ...         ...       ...\n",
       "32839  E01004738        1879  0.085649\n",
       "32840  E01004665        1743  0.013524\n",
       "32841  E01004669        1820  0.100576\n",
       "32842  E01004739        1598  0.028525\n",
       "32843  E01004741        1434  0.051046\n",
       "\n",
       "[32844 rows x 3 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_area_df"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
